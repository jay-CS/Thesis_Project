{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8796c08-7b0c-4b51-8171-78470f103f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from sklearn import metrics\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import math\n",
    "import random\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e28cfee0-1873-4557-a944-890f2d02c481",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_data_files' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/tmp/ipykernel_5237/146626440.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Total number of subjects:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_data_files\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# print(\"Total number of subjects:\", len(all_generated_data_files))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# print(all_generated_data_files)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'all_data_files' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Total number of subjects:\", len(all_data_files))\n",
    "# print(\"Total number of subjects:\", len(all_generated_data_files))\n",
    "# print(all_generated_data_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0a7e339-b04e-4c81-98b6-1796aa2b4af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KeystrokesDataset(Dataset):\n",
    "    def __init__(self, samples, labels, transform):\n",
    "        self.samples = samples\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        x = self.samples[idx]\n",
    "        x = x.transpose((1, 2 ,0))\n",
    "        x = self.transform(image=x)['image']\n",
    "        label = torch.from_numpy(self.labels[idx]).float()\n",
    "        return x, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "27c2cb46-dbda-411d-8480-2a252ca1df4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KeystrokeImageNetwork(nn.Module):\n",
    "  \n",
    "    def __init__(self):\n",
    "        super(KeystrokeImageNetwork, self).__init__()\n",
    "\n",
    "        # 10 fingers\n",
    "        # => output (x, 10)\n",
    "        self.conv1_1 = nn.Conv2d(5, 64, kernel_size=3, padding=1)\n",
    "        self.conv1_2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "\n",
    "        self.conv2_1 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.conv2_2 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "\n",
    "        # self.conv3_1 = nn.Conv2d(5, 64, kernel_size=(1,42), padding=0)\n",
    "        # self.conv4_1 = nn.Conv2d(5, 64, kernel_size=(43,1), padding=0)\n",
    "\n",
    "        # self.conv3_1 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        # self.conv3_2 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "        # self.conv3_3 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "\n",
    "        # max pooling (kernel_size, stride)\n",
    "        # self.pool = nn.AvgPool2d(2, 2)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        # fully conected layers:\n",
    "        # self.fc6 = nn.Linear(42*42*5, 512)\n",
    "        self.fc6 = nn.Linear(10*10*128, 512)\n",
    "        # self.fc6 = nn.Linear(10*10*128 + 64*43 + 64*42, 512)\n",
    "        self.fc7 = nn.Linear(512, 64)\n",
    "        self.fc8 = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x, training=True):\n",
    "        # beforeX = F.relu(self.conv3_1(x))\n",
    "        # afterX = F.relu(self.conv4_1(x))\n",
    "\n",
    "        # print(\"beofore x shape is %s, after x shape is %s\"%(beforeX.shape, afterX.shape)) #64 x 64 x 43 x 1 #64 x 64 x 1 x 42\n",
    "\n",
    "        # probelm of the sparse feature\n",
    "        # need to be fixed in the feature engineering\n",
    "        # Add two new channel \n",
    "        # beforeX = beforeX.view(-1, 64 * 43 * 1)  # represent bigraph (*) - (correspending key)\n",
    "        # afterX = afterX.view(-1, 64 * 1 * 42) # represent bigraph (corresponding key) - (*)\n",
    "\n",
    "        x = F.relu(self.conv1_1(x))\n",
    "        x = F.relu(self.conv1_2(x))\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = F.relu(self.conv2_1(x))\n",
    "        x = F.relu(self.conv2_2(x))\n",
    "        x = self.pool(x)\n",
    "\n",
    "        # x = F.relu(self.conv3_1(x))\n",
    "        # x = F.relu(self.conv3_2(x))\n",
    "        # x = F.relu(self.conv3_3(x))\n",
    "        # x = self.pool(x)\n",
    "\n",
    "        # x = x.view(-1, 42*42*5)\n",
    "        x = x.view(-1, 10*10*128)\n",
    "\n",
    "        # x = torch.cat((x, beforeX, afterX), 1)\n",
    "        # x = F.dropout(x, 0.5, training=training)\n",
    "        x = F.relu(self.fc6(x))\n",
    "        x = F.dropout(x, 0.5, training=training)\n",
    "        x = F.relu(self.fc7(x))\n",
    "        # x = F.dropout(x, 0.5, training=training)\n",
    "        x = self.fc8(x)\n",
    "\n",
    "        # x = F.log_softmax(x, dim=1)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b925c90-336e-49f2-b353-e40d8e0ddb7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#EXTRACTS REAL AND GENERATED SAMPLES\n",
    "def extractRealAndGeneratedData(user_num, all_data_files,all_generated_data_files, positive_only = False, gan_only = False):\n",
    "    positive_data = None\n",
    "    negative_data = None\n",
    "\n",
    "    positive_init = False\n",
    "    negative_init = False\n",
    "\n",
    "    positive_indices = None\n",
    "    negative_indices = None\n",
    "\n",
    "    positive_data = list()\n",
    "    negative_data = list()\n",
    "    target = user_num\n",
    "    total = 40000\n",
    "    \n",
    "    #print('**********************', target)\n",
    "    \n",
    "    if(positive_only):\n",
    "        for i, filename in enumerate(all_data_files):\n",
    "            if i == target:\n",
    "                positive_dataset = np.load(filename)\n",
    "                positive_indices = list(range(len(positive_dataset)))\n",
    "                np.random.shuffle(positive_indices)\n",
    "                positive_init = True\n",
    "                positive_data = positive_dataset[positive_indices]\n",
    "                #print(\"Current positive keystroke images Data shape is\",positive_data.shape)\n",
    "        return positive_data\n",
    "    \n",
    "    if(gan_only):\n",
    "        for i, filename in enumerate(all_generated_data_files):\n",
    "            if i == target:\n",
    "                negative_dataset = np.load(filename)\n",
    "                negative_indices = list(range(len(negative_dataset)))\n",
    "                np.random.shuffle(negative_indices)\n",
    "                negative_init = True\n",
    "                negative_data = negative_dataset[negative_indices]\n",
    "                #print(\"Current negative keystroke images Data shape is\",negative_data.shape)\n",
    "        return negative_data\n",
    "        \n",
    "    else:\n",
    "        for i, filename in enumerate(all_data_files):\n",
    "            if i == target:\n",
    "                positive_dataset = np.load(filename)\n",
    "                # positive_indices = list(range(len(positive_dataset)))\n",
    "                # np.random.shuffle(positive_indices)\n",
    "                # positive_init = True\n",
    "                # positive_data = positive_dataset[positive_indices]\n",
    "                #print(\"Current positive keystroke images Data shape is\",positive_data.shape)\n",
    "    \n",
    "        for i, filename in enumerate(all_generated_data_files):\n",
    "            if i == target:\n",
    "                negative_dataset = np.load(filename)\n",
    "                # negative_indices = list(range(len(negative_dataset)))\n",
    "                # np.random.shuffle(negative_indices)\n",
    "                # negative_init = True\n",
    "                # negative_data = negative_dataset[negative_indices]\n",
    "                #print(\"Current negative keystroke images Data shape is\",negative_data.shape)\n",
    "        return positive_dataset, negative_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ddea05fb-d709-454e-9099-c6702e8c11fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNegative(target,all_data_files):\n",
    "    positive_length = 100\n",
    "    negative_length = 35000\n",
    "    negative_data = None\n",
    "    negative_init = False\n",
    "        \n",
    "    for i, filename in enumerate(all_data_files):\n",
    "            if i != target:\n",
    "                negative_dataset = np.load(filename)\n",
    "                nega_len = len(negative_dataset)\n",
    "                if random.randint(0, 1) == 0:\n",
    "                    smaple_len = math.floor(nega_len/negative_length * positive_length)\n",
    "                else:\n",
    "                    smaple_len = math.ceil(nega_len/negative_length * positive_length)\n",
    "                negative_indices = list(range(nega_len))\n",
    "\n",
    "                if not negative_init:\n",
    "                    negative_data = negative_dataset[negative_indices[:smaple_len]]\n",
    "                    negative_init = True\n",
    "                else:\n",
    "                    extend_sameple = negative_dataset[negative_indices[:smaple_len]]\n",
    "                    negative_data = np.concatenate((negative_data, extend_sameple), axis=0)\n",
    "    #print(\"Negative Data Shape:\",negative_data.shape)\n",
    "    return negative_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a3154400-42c2-42ec-93c5-25db34dc1583",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish\n",
      "(64, 5, 42, 42)\n",
      "(64, 5, 42, 42)\n",
      "(1, 5, 42, 42)\n",
      "(64, 5, 42, 42)\n",
      "(64, 5, 42, 42)\n",
      "(64, 5, 42, 42)\n",
      "(0,)\n",
      "(64, 5, 42, 42)\n",
      "(64, 5, 42, 42)\n",
      "(64, 5, 42, 42)\n",
      "(0,)\n",
      "(64, 5, 42, 42)\n",
      "(64, 5, 42, 42)\n",
      "(64, 5, 42, 42)\n",
      "(64, 5, 42, 42)\n",
      "(64, 5, 42, 42)\n",
      "(64, 5, 42, 42)\n",
      "(64, 5, 42, 42)\n",
      "(64, 5, 42, 42)\n",
      "(64, 5, 42, 42)\n",
      "(64, 5, 42, 42)\n",
      "(64, 5, 42, 42)\n",
      "(64, 5, 42, 42)\n",
      "(64, 5, 42, 42)\n",
      "(64, 5, 42, 42)\n",
      "(64, 5, 42, 42)\n",
      "(64, 5, 42, 42)\n",
      "(64, 5, 42, 42)\n",
      "(60, 5, 42, 42)\n",
      "(0,)\n",
      "(64, 5, 42, 42)\n",
      "(64, 5, 42, 42)\n",
      "(0,)\n",
      "(64, 5, 42, 42)\n",
      "(64, 5, 42, 42)\n",
      "(64, 5, 42, 42)\n",
      "(64, 5, 42, 42)\n",
      "(64, 5, 42, 42)\n",
      "(32, 5, 42, 42)\n",
      "(0,)\n",
      "(64, 5, 42, 42)\n",
      "(64, 5, 42, 42)\n",
      "(64, 5, 42, 42)\n",
      "(64, 5, 42, 42)\n",
      "(64, 5, 42, 42)\n",
      "(64, 5, 42, 42)\n",
      "(64, 5, 42, 42)\n",
      "(64, 5, 42, 42)\n",
      "(0,)\n",
      "(64, 5, 42, 42)\n",
      "(64, 5, 42, 42)\n",
      "(64, 5, 42, 42)\n",
      "(60, 5, 42, 42)\n",
      "(64, 5, 42, 42)\n",
      "(64, 5, 42, 42)\n",
      "(64, 5, 42, 42)\n",
      "(64, 5, 42, 42)\n",
      "(64, 5, 42, 42)\n",
      "(64, 5, 42, 42)\n",
      "(64, 5, 42, 42)\n",
      "(64, 5, 42, 42)\n",
      "(64, 5, 42, 42)\n",
      "(0,)\n",
      "(64, 5, 42, 42)\n",
      "(64, 5, 42, 42)\n",
      "(64, 5, 42, 42)\n",
      "(64, 5, 42, 42)\n",
      "(64, 5, 42, 42)\n",
      "(56, 5, 42, 42)\n",
      "(64, 5, 42, 42)\n",
      "(0,)\n",
      "(0,)\n",
      "(64, 5, 42, 42)\n",
      "(64, 5, 42, 42)\n",
      "(64, 5, 42, 42)\n",
      "finish\n",
      "(64, 5, 42, 42)\n",
      "(56, 5, 42, 42)\n",
      "(64, 5, 42, 42)\n",
      "(64, 5, 42, 42)\n",
      "(48, 5, 42, 42)\n",
      "(0,)\n",
      "(0,)\n",
      "(59, 5, 42, 42)\n",
      "(64, 5, 42, 42)\n",
      "(58, 5, 42, 42)\n",
      "(0,)\n",
      "(54, 5, 42, 42)\n",
      "(59, 5, 42, 42)\n",
      "(62, 5, 42, 42)\n",
      "(0,)\n",
      "(60, 5, 42, 42)\n",
      "(64, 5, 42, 42)\n",
      "(62, 5, 42, 42)\n",
      "(54, 5, 42, 42)\n",
      "(53, 5, 42, 42)\n",
      "(46, 5, 42, 42)\n",
      "(52, 5, 42, 42)\n",
      "(52, 5, 42, 42)\n",
      "(54, 5, 42, 42)\n",
      "(60, 5, 42, 42)\n",
      "(64, 5, 42, 42)\n",
      "(64, 5, 42, 42)\n",
      "(61, 5, 42, 42)\n",
      "(40, 5, 42, 42)\n",
      "(1, 5, 42, 42)\n",
      "(0,)\n",
      "(64, 5, 42, 42)\n",
      "(0,)\n",
      "(57, 5, 42, 42)\n",
      "(62, 5, 42, 42)\n",
      "(59, 5, 42, 42)\n",
      "(63, 5, 42, 42)\n",
      "(36, 5, 42, 42)\n",
      "(56, 5, 42, 42)\n",
      "(50, 5, 42, 42)\n",
      "(54, 5, 42, 42)\n",
      "(53, 5, 42, 42)\n",
      "(56, 5, 42, 42)\n",
      "(60, 5, 42, 42)\n",
      "(60, 5, 42, 42)\n",
      "(0,)\n",
      "(0,)\n",
      "(0,)\n",
      "(52, 5, 42, 42)\n",
      "(58, 5, 42, 42)\n",
      "(64, 5, 42, 42)\n",
      "(56, 5, 42, 42)\n",
      "(40, 5, 42, 42)\n",
      "(56, 5, 42, 42)\n",
      "(64, 5, 42, 42)\n",
      "(63, 5, 42, 42)\n",
      "(0,)\n",
      "(64, 5, 42, 42)\n",
      "(64, 5, 42, 42)\n",
      "(0,)\n",
      "(63, 5, 42, 42)\n",
      "(64, 5, 42, 42)\n",
      "(0,)\n",
      "(58, 5, 42, 42)\n",
      "(55, 5, 42, 42)\n",
      "(63, 5, 42, 42)\n",
      "(57, 5, 42, 42)\n",
      "(56, 5, 42, 42)\n",
      "(60, 5, 42, 42)\n",
      "(50, 5, 42, 42)\n",
      "(0,)\n",
      "(57, 5, 42, 42)\n",
      "(64, 5, 42, 42)\n",
      "(64, 5, 42, 42)\n",
      "(55, 5, 42, 42)\n",
      "finish\n",
      "(55, 5, 42, 42)\n",
      "(42, 5, 42, 42)\n",
      "(48, 5, 42, 42)\n",
      "(56, 5, 42, 42)\n",
      "(36, 5, 42, 42)\n",
      "(45, 5, 42, 42)\n",
      "(0,)\n",
      "(44, 5, 42, 42)\n",
      "(63, 5, 42, 42)\n",
      "(33, 5, 42, 42)\n",
      "(0,)\n",
      "(40, 5, 42, 42)\n",
      "(44, 5, 42, 42)\n",
      "(47, 5, 42, 42)\n",
      "(49, 5, 42, 42)\n",
      "(45, 5, 42, 42)\n",
      "(54, 5, 42, 42)\n",
      "(46, 5, 42, 42)\n",
      "(40, 5, 42, 42)\n",
      "(0,)\n",
      "(0,)\n",
      "(39, 5, 42, 42)\n",
      "(40, 5, 42, 42)\n",
      "(41, 5, 42, 42)\n",
      "(45, 5, 42, 42)\n",
      "(54, 5, 42, 42)\n",
      "(48, 5, 42, 42)\n",
      "(45, 5, 42, 42)\n",
      "(29, 5, 42, 42)\n",
      "(38, 5, 42, 42)\n",
      "(0,)\n",
      "(48, 5, 42, 42)\n",
      "(0,)\n",
      "(43, 5, 42, 42)\n",
      "(2, 5, 42, 42)\n",
      "(44, 5, 42, 42)\n",
      "(47, 5, 42, 42)\n",
      "(42, 5, 42, 42)\n",
      "(0,)\n",
      "(38, 5, 42, 42)\n",
      "(2, 5, 42, 42)\n",
      "(0,)\n",
      "(42, 5, 42, 42)\n",
      "(44, 5, 42, 42)\n",
      "(44, 5, 42, 42)\n",
      "(0,)\n",
      "(39, 5, 42, 42)\n",
      "(0,)\n",
      "(39, 5, 42, 42)\n",
      "(44, 5, 42, 42)\n",
      "(54, 5, 42, 42)\n",
      "(42, 5, 42, 42)\n",
      "(29, 5, 42, 42)\n",
      "(41, 5, 42, 42)\n",
      "(48, 5, 42, 42)\n",
      "(47, 5, 42, 42)\n",
      "(0,)\n",
      "(51, 5, 42, 42)\n",
      "(50, 5, 42, 42)\n",
      "(0,)\n",
      "(33, 5, 42, 42)\n",
      "(55, 5, 42, 42)\n",
      "(0,)\n",
      "(44, 5, 42, 42)\n",
      "(41, 5, 42, 42)\n",
      "(47, 5, 42, 42)\n",
      "(43, 5, 42, 42)\n",
      "(38, 5, 42, 42)\n",
      "(44, 5, 42, 42)\n",
      "(37, 5, 42, 42)\n",
      "(0,)\n",
      "(43, 5, 42, 42)\n",
      "(58, 5, 42, 42)\n",
      "(50, 5, 42, 42)\n",
      "(41, 5, 42, 42)\n",
      "finish\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/tmp/ipykernel_5237/262012622.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m \u001b[0mtestCNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/tmp/ipykernel_5237/262012622.py\u001b[0m in \u001b[0;36mtestCNN\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m                     \u001b[0;31m#Loading and testing model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m                     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_cnns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m                     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m                     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    787\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnpicklingError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mUNSAFE_MESSAGE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    790\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mweights_only\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1129\u001b[0m     \u001b[0munpickler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUnpicklerWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersistent_load\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpersistent_load\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1131\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_loaded_sparse_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mpersistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m   1099\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloaded_storages\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m             \u001b[0mnbytes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumel\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_element_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1101\u001b[0;31m             \u001b[0mload_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_maybe_decode_ascii\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1103\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloaded_storages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload_tensor\u001b[0;34m(dtype, numel, key, location)\u001b[0m\n\u001b[1;32m   1077\u001b[0m         \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'data/{key}'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1078\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1079\u001b[0;31m         \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_storage_from_record\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUntypedStorage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muntyped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1080\u001b[0m         \u001b[0;31m# TODO: Once we decide to break serialization FC, we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m         \u001b[0;31m# stop wrapping with TypedStorage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## EVALUATING GENERATED DATA  \n",
    "model = KeystrokeImageNetwork()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "knum = 0\n",
    "# list of test results to save and average at the end (CNN [50,75,100] vs [WGAN,DCGAN,CGAN][50,75,100]) 27 different lists\n",
    "cnn_test_data = {'WGAN':[],'DCGAN':[],'CGAN':[]}\n",
    "cnn_pos_data = list() # cnn tested on only positive (\"1) data from dataset\n",
    "cnn_gan_data = list() # cnn tested on gan data (\"1\") labeled real\n",
    "cnn_pos_and_gan_data = list()# cnn tested on positive and real data labeled 1\n",
    "cnn_pos_vs_gan_data = list() # cnn tested on positive data (\"1\") vs generated data labeled (\"0\")\n",
    "cnn_pos_vs_neg_data = list() # \n",
    "cnn_gan_vs_neg_data = list() # \n",
    "cnn_pos_and_gan_vs_neg_data = list() #\n",
    "\n",
    "def testCNN():\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "        for gan in ['CGAN','DCGAN']:\n",
    "\n",
    "\n",
    "            for keystroke_num in ['50','75','100']:\n",
    "                print(\"finish\")\n",
    "                cnn_pos_data = list() # cnn tested on only positive (\"1) data from dataset\n",
    "                cnn_gan_data = list() # cnn tested on gan data (\"1\") labeled real\n",
    "                cnn_pos_and_gan_data = list()# cnn tested on positive and real data labeled 1\n",
    "                cnn_pos_vs_gan_data = list() # cnn tested on positive data (\"1\") vs generated data labeled (\"0\")\n",
    "                cnn_pos_vs_neg_data = list() # \n",
    "                cnn_gan_vs_neg_data = list() # \n",
    "                cnn_pos_and_gan_vs_neg_data = list() #\n",
    "                path = r\"/home/jupyter/src/Thesis_Project/Data/\"+keystroke_num\n",
    "                all_data_files = glob.glob(path + \"/*\")\n",
    "                all_data_files.sort()\n",
    "                path = r\"/home/jupyter/src/Thesis_Project/\"+gan+\"_data/\"+keystroke_num\n",
    "                all_generated_data_files = glob.glob(path + \"/*\")\n",
    "                all_generated_data_files.sort()\n",
    "                path = r\"/home/jupyter/src/Thesis_Project/CNN_\" +keystroke_num+\"_models\"\n",
    "                all_cnns = glob.glob(path + \"/*\")\n",
    "                all_cnns.sort()\n",
    "\n",
    "                for k in range(75):\n",
    "                    # POS + GAN (i.e. both are labeled ones)\n",
    "                    positive_data, gan_data = extractRealAndGeneratedData(k,all_data_files,all_generated_data_files)\n",
    "                    total_dataset = np.concatenate((positive_data, gan_data), axis = 0)\n",
    "                    total_labels = np.concatenate((\n",
    "                      np.ones((len(positive_data), 1)), \n",
    "                      np.ones((len(gan_data), 1))\n",
    "                    ), axis=0)\n",
    "                    test_transform = getTransform(total_dataset)\n",
    "                    test_dataset = KeystrokesDataset(total_dataset, total_labels, test_transform)\n",
    "                    test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "                    #Loading and testing model \n",
    "                    model.load_state_dict(torch.load(all_cnns[k]))\n",
    "                    model.to(device)\n",
    "                    model.eval()\n",
    "\n",
    "                    \n",
    "                    test_acc, eer = evaluate(model,test_dataloader)\n",
    "                    cnn_pos_and_gan_data.append([test_acc,eer])\n",
    "\n",
    "\n",
    "                    # POS ONLY\n",
    "                    total_dataset = positive_data\n",
    "                    total_labels = np.ones((len(positive_data), 1))\n",
    "                    test_transform = getTransform(total_dataset)\n",
    "                    test_dataset = KeystrokesDataset(total_dataset, total_labels, test_transform)\n",
    "                    test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=True)\n",
    "                    test_acc, eer = evaluate(model,test_dataloader)\n",
    "                    # print('Accuracy of the network on the test inputs: %f %%' % (test_acc))\n",
    "                    cnn_pos_data.append([test_acc,eer])\n",
    "\n",
    "\n",
    "                    # GAN ONLY\n",
    "                    total_dataset = gan_data\n",
    "                    total_labels = np.ones((len(gan_data), 1))\n",
    "                    test_transform = getTransform(total_dataset)\n",
    "                    test_dataset = KeystrokesDataset(total_dataset, total_labels, test_transform)\n",
    "                    test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=True)\n",
    "                    test_acc, eer = evaluate(model,test_dataloader)\n",
    "                    # print('Accuracy of the network on the test inputs: %f %%' % (test_acc))\n",
    "                    cnn_gan_data.append([test_acc,eer])\n",
    "                    # Test each CNN against Generated Data\n",
    "                    if(gan == \"CGAN\"):\n",
    "                        test_acc, eer = evaluate(model,test_dataloader,keystroke_num,k,save_res=True)\n",
    "                        cnn_pos_and_gan_data.append([test_acc,eer])\n",
    "                    else:\n",
    "                        test_acc, eer = evaluate(model,test_dataloader)\n",
    "                        cnn_pos_and_gan_data.append([test_acc,eer])\n",
    "\n",
    "\n",
    "                    # POS vs GAN ONLY\n",
    "                    total_dataset = np.concatenate((positive_data, gan_data), axis = 0)\n",
    "                    total_labels = np.concatenate((\n",
    "                      np.ones((len(positive_data), 1)), \n",
    "                      np.zeros((len(gan_data), 1))\n",
    "                    ), axis=0)\n",
    "                    test_transform = getTransform(total_dataset)\n",
    "                    test_dataset = KeystrokesDataset(total_dataset, total_labels, test_transform)\n",
    "                    test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=True)\n",
    "                    test_acc, eer = evaluate(model,test_dataloader)\n",
    "                    # print('Accuracy of the network on the test inputs: %f %%' % (test_acc))\n",
    "                    cnn_pos_vs_gan_data.append([test_acc,eer])\n",
    "\n",
    "                    # GRAB  NEGATIVE DATA\n",
    "                    negative_data = getNegative(k,all_data_files)\n",
    "\n",
    "                    # POS vs NEG \n",
    "                    total_dataset = np.concatenate((positive_data, negative_data), axis = 0)\n",
    "                    total_labels = np.concatenate((\n",
    "                      np.ones((len(positive_data), 1)), \n",
    "                      np.zeros((len(negative_data), 1))\n",
    "                    ), axis=0)\n",
    "                    test_transform = getTransform(total_dataset)\n",
    "                    test_dataset = KeystrokesDataset(total_dataset, total_labels, test_transform)\n",
    "                    test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=True)\n",
    "                    test_acc, eer = evaluate(model,test_dataloader)\n",
    "                    # print('Accuracy of the network on the test inputs: %f %%' % (test_acc))\n",
    "                    cnn_pos_vs_neg_data.append([test_acc,eer])\n",
    "\n",
    "\n",
    "                    # GAN vs NEG\n",
    "                    total_dataset = np.concatenate((gan_data, negative_data), axis = 0)\n",
    "                    total_labels = np.concatenate((\n",
    "                      np.ones((len(gan_data), 1)), \n",
    "                      np.zeros((len(negative_data), 1))\n",
    "                    ), axis=0)\n",
    "                    test_transform = getTransform(total_dataset)\n",
    "                    test_dataset = KeystrokesDataset(total_dataset, total_labels, test_transform)\n",
    "                    test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=True)\n",
    "                    test_acc, eer = evaluate(model,test_dataloader)\n",
    "                    # print('Accuracy of the network on the test inputs: %f %%' % (test_acc))\n",
    "                    cnn_gan_vs_neg_data.append([test_acc,eer])\n",
    "\n",
    "\n",
    "                    # POS + GAN vs NEG \n",
    "                    total_dataset = np.concatenate((positive_data,np.concatenate((gan_data, negative_data), axis = 0)),axis=0)\n",
    "                    total_labels = np.concatenate((\n",
    "                      np.ones((len(positive_data)+len(gan_data), 1)), \n",
    "                      np.zeros((len(negative_data), 1))\n",
    "                    ), axis=0)\n",
    "                    test_transform = getTransform(total_dataset)\n",
    "                    test_dataset = KeystrokesDataset(total_dataset, total_labels, test_transform)\n",
    "                    test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=True)\n",
    "                    test_acc, eer = evaluate(model,test_dataloader)\n",
    "                    # print('Accuracy of the network on the test inputs: %f %%' % (test_acc))\n",
    "                    cnn_pos_and_gan_vs_neg_data.append([test_acc, eer])\n",
    "\n",
    "\n",
    "\n",
    "                cnn_test_data[gan].append([cnn_pos_data,cnn_gan_data,cnn_pos_and_gan_data,cnn_pos_vs_gan_data,cnn_pos_vs_neg_data,cnn_gan_vs_neg_data, cnn_pos_and_gan_vs_neg_data])\n",
    "\n",
    "\n",
    "testCNN()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e619327-a74d-42e8-ae5f-b2b2506a695f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Here\")\n",
    "test_name = [\"cnn_pos_data\\t\\t\\t\",\"cnn_gan_data\\t\\t\\t\",\"cnn_pos_and_gan_data\\t\\t\",\"cnn_pos_vs_gan_data\\t\\t\",\"cnn_pos_vs_neg_data\\t\\t\",\"cnn_gan_vs_neg_data\\t\\t\", \"cnn_pos_and_gan_vs_neg_data\\t\"]\n",
    "knum = ['50','75','100']\n",
    "print(\"~~~~~~~~~RESULTS~~~~~~~~~\")\n",
    "for g in ['WGAN','CGAN','DCGAN']:\n",
    "    k = 0\n",
    "    for i in cnn_test_data[g]:\n",
    "        t = 0\n",
    "        for tests in i:\n",
    "            tests = np.array(tests)\n",
    "            print(g +'\\t',knum[k]+\" keystrokes\\t\",test_name[t] + \"AVERAGE:\",round(sum(tests[:,0])/len(tests[:,0]),3),\" , EER:\", round(sum(tests[:,1])/len(tests[:,1]),3))\n",
    "            t += 1\n",
    "        k += 1\n",
    "        print(\"\\n\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20150c78-927c-4a1a-a70e-b2cc1d2b1973",
   "metadata": {},
   "source": [
    "#### positive_indices = list(range(3))\n",
    "positive = [[1,2,3],[1,1,1],[2,2,2]]\n",
    "np.random.shuffle(positive_indices)\n",
    "positive_data = positive[positive_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac164a28-f0f6-42ed-9e36-4caee78f48b8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/tmp/ipykernel_2330/1509070237.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m#Loading and testing model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_cnns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "import os \n",
    "cnn_pos_data = list() # cnn tested on only positive (\"1) data from dataset\n",
    "cnn_gan_data = list() # cnn tested on gan data (\"1\") labeled real\n",
    "cnn_pos_and_gan_data = list()# cnn tested on positive and real data labeled 1\n",
    "cnn_pos_vs_gan_data = list() # cnn tested on positive data (\"1\") vs generated data labeled (\"0\")\n",
    "cnn_pos_vs_neg_data = list() # \n",
    "cnn_gan_vs_neg_data = list() # \n",
    "cnn_pos_and_gan_vs_neg_data = list() #\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "path = r\"/home/jupyter/src/Thesis_Project/Data/100\"\n",
    "all_data_files = glob.glob(path + \"/*\")\n",
    "all_data_files.sort()\n",
    "path = r\"/home/jupyter/src/Thesis_Project/WGAN_data/100\"\n",
    "all_generated_data_files = glob.glob(path + \"/*\")\n",
    "all_generated_data_files.sort()\n",
    "path = r\"/home/jupyter/src/Thesis_Project/CNN_100_models\"\n",
    "all_cnns = glob.glob(path + \"/*\")\n",
    "all_cnns.sort()\n",
    "# print(all_generated_data_files)\n",
    "    \n",
    "\n",
    "#Loading and testing model \n",
    "model.load_state_dict(torch.load(all_cnns[0]))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "l=0\n",
    "for k in range(75):\n",
    "    # POS + GAN (i.e. both are labeled ones)\n",
    "    positive_data, g = extractRealAndGeneratedData(k,all_data_files,all_generated_data_files)\n",
    "    p, gan_data = extractRealAndGeneratedData(k,all_data_files,all_generated_data_files)\n",
    "    # are_same = np.array_equal(positive_data,p)\n",
    "    # are_same1 = np.array_equal(gan_data,g)\n",
    "    # if are_same:\n",
    "    #     print(\"The arrays are the same.\")\n",
    "    # else:\n",
    "    #     print(\"The arrays are different.\")\n",
    "    # if are_same1:\n",
    "    #     print(\"The arrays are the same.\")\n",
    "    # else:\n",
    "    #     print(\"The arrays are different.\")  \n",
    "\n",
    "    total_dataset = gan_data\n",
    "    total_labels = np.ones((len(gan_data), 1))\n",
    "    # total_dataset = np.concatenate((positive_data, gan_data), axis = 0)\n",
    "    # total_labels = np.concatenate((\n",
    "    #   np.ones((len(positive_data), 1)), \n",
    "    #   np.ones((len(gan_data), 1))\n",
    "    # ), axis=0)\n",
    "    test_transform = getTransform(total_dataset)\n",
    "    test_dataset = KeystrokesDataset(total_dataset, total_labels, test_transform)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "    #Loading and testing model \n",
    "    model.load_state_dict(torch.load(all_cnns[k]))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    test_acc = evaluate(model,test_dataloader)\n",
    "    l += test_acc\n",
    "\n",
    "\n",
    "    print('Accuracy of the network on the test inputs: %f %%' % (test_acc))\n",
    "print('Accuracy of the network on the test inputs: %f %%' % (l/48))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b263caac-70bd-41c8-bafc-3fd0f365636a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0_data.npy', '1_data.npy', '2_data.npy', '3_data.npy', '4_data.npy', '5_data.npy', '6_data.npy', '7_data.npy', '8_data.npy', '9_data.npy', '10_data.npy', '11_data.npy', '12_data.npy', '13_data.npy', '14_data.npy', '15_data.npy', '16_data.npy', '17_data.npy', '18_data.npy', '19_data.npy', '20_data.npy', '21_data.npy', '22_data.npy', '23_data.npy', '24_data.npy', '25_data.npy', '26_data.npy', '27_data.npy', '28_data.npy', '29_data.npy', '30_data.npy', '31_data.npy', '32_data.npy', '33_data.npy', '34_data.npy', '35_data.npy', '36_data.npy', '37_data.npy', '38_data.npy', '39_data.npy', '40_data.npy', '41_data.npy', '42_data.npy', '43_data.npy', '44_data.npy', '45_data.npy', '46_data.npy', '47_data.npy', '48_data.npy', '49_data.npy', '50_data.npy', '51_data.npy', '52_data.npy', '53_data.npy', '54_data.npy', '55_data.npy', '56_data.npy', '57_data.npy', '58_data.npy', '59_data.npy', '60_data.npy', '61_data.npy', '62_data.npy', '63_data.npy', '64_data.npy', '65_data.npy', '66_data.npy', '67_data.npy', '68_data.npy', '69_data.npy', '70_data.npy', '71_data.npy', '72_data.npy', '73_data.npy', '74_data.npy']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "directory = '/home/jupyter/src/Thesis_Project/CGAN_CNN_data/50/'\n",
    "prefix = ''\n",
    "extension = '_data.npy'\n",
    "\n",
    "#Get the list of files to be renamed\n",
    "files = os.listdir(directory)\n",
    "files = [f for f in files if f.endswith(extension)]\n",
    "# files = sorted(files, key=lambda x: int(x.split('.')[0].split('_')[-1]))\n",
    "files = sorted(files, key=lambda x: int(x.split('.')[0].split('_')[0]))\n",
    "print(files)\n",
    "\n",
    "#Rename the files\n",
    "for i, f in enumerate(files):\n",
    "    old_name = os.path.join(directory, f)\n",
    "    new_name = os.path.join(directory, prefix + str(i).zfill(3) + extension)\n",
    "    os.rename(old_name, new_name)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81076fe9-a770-4ced-8664-ade0ba35d269",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model,test_dataloader,keystroke='50',user_num = 0,save_res=False):\n",
    "    correct = 0.0\n",
    "    total = 0.0\n",
    "\n",
    "    scores = []\n",
    "    y = []\n",
    "    correct_predictions = []\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(test_dataloader):\n",
    "            inputs, labels = data\n",
    "            #images = images.to(device).half() # uncomment for half precision model\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            #Passes input through CNN and generates output\n",
    "            outputs = model.forward(inputs, training=False)\n",
    "            total += labels.size(0)\n",
    "\n",
    "            scores.extend(outputs.cpu().numpy().reshape(len(outputs)))\n",
    "            y.extend(labels.cpu().numpy().reshape(len(labels)))\n",
    "            \n",
    "            #scores are then aggregated and averaged\n",
    "            predicted = (outputs > 0.3).float()\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "            \n",
    "            #save the correctly predicted values\n",
    "            for j in range(len(predicted)):\n",
    "                if predicted[j] == labels[j]:\n",
    "                    correct_predictions.append(inputs[j].cpu().numpy())\n",
    "                    \n",
    "           \n",
    "\n",
    "    if(save_res):\n",
    "        print(np.array(correct_predictions).shape)\n",
    "        np.save(\"/home/jupyter/src/Thesis_Project/CGAN_CNN_data/\"+keystroke+\"/\" + str(user_num) + \"_data.npy\", np.array(correct_predictions))\n",
    "    test_acc = (100.0 * correct) / total\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y, scores, pos_label=1)\n",
    "    fnr = 1 - tpr\n",
    "    eer_threshold = thresholds[np.argmin(np.absolute(tpr-(1 - fpr)))]\n",
    "    eer = fpr[np.argmin(np.absolute(tpr-(1 - fpr)))]\n",
    "    return test_acc, eer\n",
    "\n",
    "def getTransform(total_dataset):\n",
    "    mean = 0.\n",
    "    std = 0.\n",
    "\n",
    "    # batch size (the last batch can have smaller size!)\n",
    "    batch_samples = len(total_dataset) \n",
    "    images = total_dataset.reshape(batch_samples, 5, -1)\n",
    "    mean += images.mean(2).sum(0)\n",
    "    std += images.std(2).sum(0)\n",
    "\n",
    "    mean /= len(total_dataset)\n",
    "    std /= len(total_dataset)\n",
    "\n",
    "\n",
    "    test_transform = A.Compose([\n",
    "    A.Normalize(mean=mean.tolist(), std=std.tolist(), max_pixel_value=1.0, p=1.0),\n",
    "    ToTensorV2(p=1.0),\n",
    "    ], p=1)\n",
    "    return test_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cac72f0f-a2ec-4fbc-ac6d-1fd5bbef6dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 3]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2977cb-a703-43cc-bbff-16f6c0ba3e36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.1-13.m103",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-13:m103"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
