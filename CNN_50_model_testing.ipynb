{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8796c08-7b0c-4b51-8171-78470f103f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from sklearn import metrics\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import math\n",
    "import random\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e28cfee0-1873-4557-a944-890f2d02c481",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_data_files' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/tmp/ipykernel_2330/146626440.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Total number of subjects:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_data_files\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# print(\"Total number of subjects:\", len(all_generated_data_files))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# print(all_generated_data_files)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'all_data_files' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Total number of subjects:\", len(all_data_files))\n",
    "# print(\"Total number of subjects:\", len(all_generated_data_files))\n",
    "# print(all_generated_data_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0a7e339-b04e-4c81-98b6-1796aa2b4af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KeystrokesDataset(Dataset):\n",
    "    def __init__(self, samples, labels, transform):\n",
    "        self.samples = samples\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        x = self.samples[idx]\n",
    "        x = x.transpose((1, 2 ,0))\n",
    "        x = self.transform(image=x)['image']\n",
    "        label = torch.from_numpy(self.labels[idx]).float()\n",
    "        return x, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27c2cb46-dbda-411d-8480-2a252ca1df4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KeystrokeImageNetwork(nn.Module):\n",
    "  \n",
    "    def __init__(self):\n",
    "        super(KeystrokeImageNetwork, self).__init__()\n",
    "\n",
    "        # 10 fingers\n",
    "        # => output (x, 10)\n",
    "        self.conv1_1 = nn.Conv2d(5, 64, kernel_size=3, padding=1)\n",
    "        self.conv1_2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "\n",
    "        self.conv2_1 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.conv2_2 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "\n",
    "        # self.conv3_1 = nn.Conv2d(5, 64, kernel_size=(1,42), padding=0)\n",
    "        # self.conv4_1 = nn.Conv2d(5, 64, kernel_size=(43,1), padding=0)\n",
    "\n",
    "        # self.conv3_1 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        # self.conv3_2 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "        # self.conv3_3 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "\n",
    "        # max pooling (kernel_size, stride)\n",
    "        # self.pool = nn.AvgPool2d(2, 2)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        # fully conected layers:\n",
    "        # self.fc6 = nn.Linear(42*42*5, 512)\n",
    "        self.fc6 = nn.Linear(10*10*128, 512)\n",
    "        # self.fc6 = nn.Linear(10*10*128 + 64*43 + 64*42, 512)\n",
    "        self.fc7 = nn.Linear(512, 64)\n",
    "        self.fc8 = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x, training=True):\n",
    "        # beforeX = F.relu(self.conv3_1(x))\n",
    "        # afterX = F.relu(self.conv4_1(x))\n",
    "\n",
    "        # print(\"beofore x shape is %s, after x shape is %s\"%(beforeX.shape, afterX.shape)) #64 x 64 x 43 x 1 #64 x 64 x 1 x 42\n",
    "\n",
    "        # probelm of the sparse feature\n",
    "        # need to be fixed in the feature engineering\n",
    "        # Add two new channel \n",
    "        # beforeX = beforeX.view(-1, 64 * 43 * 1)  # represent bigraph (*) - (correspending key)\n",
    "        # afterX = afterX.view(-1, 64 * 1 * 42) # represent bigraph (corresponding key) - (*)\n",
    "\n",
    "        x = F.relu(self.conv1_1(x))\n",
    "        x = F.relu(self.conv1_2(x))\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = F.relu(self.conv2_1(x))\n",
    "        x = F.relu(self.conv2_2(x))\n",
    "        x = self.pool(x)\n",
    "\n",
    "        # x = F.relu(self.conv3_1(x))\n",
    "        # x = F.relu(self.conv3_2(x))\n",
    "        # x = F.relu(self.conv3_3(x))\n",
    "        # x = self.pool(x)\n",
    "\n",
    "        # x = x.view(-1, 42*42*5)\n",
    "        x = x.view(-1, 10*10*128)\n",
    "\n",
    "        # x = torch.cat((x, beforeX, afterX), 1)\n",
    "        # x = F.dropout(x, 0.5, training=training)\n",
    "        x = F.relu(self.fc6(x))\n",
    "        x = F.dropout(x, 0.5, training=training)\n",
    "        x = F.relu(self.fc7(x))\n",
    "        # x = F.dropout(x, 0.5, training=training)\n",
    "        x = self.fc8(x)\n",
    "\n",
    "        # x = F.log_softmax(x, dim=1)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b925c90-336e-49f2-b353-e40d8e0ddb7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#EXTRACTS REAL AND GENERATED SAMPLES\n",
    "def extractRealAndGeneratedData(user_num, all_data_files,all_generated_data_files, positive_only = False, gan_only = False):\n",
    "    positive_data = None\n",
    "    negative_data = None\n",
    "\n",
    "    positive_init = False\n",
    "    negative_init = False\n",
    "\n",
    "    positive_indices = None\n",
    "    negative_indices = None\n",
    "\n",
    "    positive_data = list()\n",
    "    negative_data = list()\n",
    "    target = user_num\n",
    "    total = 40000\n",
    "    \n",
    "    #print('**********************', target)\n",
    "    \n",
    "    if(positive_only):\n",
    "        for i, filename in enumerate(all_data_files):\n",
    "            if i == target:\n",
    "                positive_dataset = np.load(filename)\n",
    "                positive_indices = list(range(len(positive_dataset)))\n",
    "                np.random.shuffle(positive_indices)\n",
    "                positive_init = True\n",
    "                positive_data = positive_dataset[positive_indices]\n",
    "                #print(\"Current positive keystroke images Data shape is\",positive_data.shape)\n",
    "        return positive_data\n",
    "    \n",
    "    if(gan_only):\n",
    "        for i, filename in enumerate(all_generated_data_files):\n",
    "            if i == target:\n",
    "                negative_dataset = np.load(filename)\n",
    "                negative_indices = list(range(len(negative_dataset)))\n",
    "                np.random.shuffle(negative_indices)\n",
    "                negative_init = True\n",
    "                negative_data = negative_dataset[negative_indices]\n",
    "                #print(\"Current negative keystroke images Data shape is\",negative_data.shape)\n",
    "        return negative_data\n",
    "        \n",
    "    else:\n",
    "        for i, filename in enumerate(all_data_files):\n",
    "            if i == target:\n",
    "                positive_dataset = np.load(filename)\n",
    "                # positive_indices = list(range(len(positive_dataset)))\n",
    "                # np.random.shuffle(positive_indices)\n",
    "                # positive_init = True\n",
    "                # positive_data = positive_dataset[positive_indices]\n",
    "                #print(\"Current positive keystroke images Data shape is\",positive_data.shape)\n",
    "    \n",
    "        for i, filename in enumerate(all_generated_data_files):\n",
    "            if i == target:\n",
    "                negative_dataset = np.load(filename)\n",
    "                # negative_indices = list(range(len(negative_dataset)))\n",
    "                # np.random.shuffle(negative_indices)\n",
    "                # negative_init = True\n",
    "                # negative_data = negative_dataset[negative_indices]\n",
    "                #print(\"Current negative keystroke images Data shape is\",negative_data.shape)\n",
    "        return positive_dataset, negative_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ddea05fb-d709-454e-9099-c6702e8c11fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNegative(target,all_data_files):\n",
    "    positive_length = 100\n",
    "    negative_length = 35000\n",
    "    negative_data = None\n",
    "    negative_init = False\n",
    "        \n",
    "    for i, filename in enumerate(all_data_files):\n",
    "            if i != target:\n",
    "                negative_dataset = np.load(filename)\n",
    "                nega_len = len(negative_dataset)\n",
    "                if random.randint(0, 1) == 0:\n",
    "                    smaple_len = math.floor(nega_len/negative_length * positive_length)\n",
    "                else:\n",
    "                    smaple_len = math.ceil(nega_len/negative_length * positive_length)\n",
    "                negative_indices = list(range(nega_len))\n",
    "\n",
    "                if not negative_init:\n",
    "                    negative_data = negative_dataset[negative_indices[:smaple_len]]\n",
    "                    negative_init = True\n",
    "                else:\n",
    "                    extend_sameple = negative_dataset[negative_indices[:smaple_len]]\n",
    "                    negative_data = np.concatenate((negative_data, extend_sameple), axis=0)\n",
    "    #print(\"Negative Data Shape:\",negative_data.shape)\n",
    "    return negative_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a3154400-42c2-42ec-93c5-25db34dc1583",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish\n",
      "finish\n"
     ]
    }
   ],
   "source": [
    "## EVALUATING GENERATED DATA  \n",
    "model = KeystrokeImageNetwork()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "knum = 0\n",
    "# list of test results to save and average at the end (CNN [50,75,100] vs [WGAN,DCGAN,CGAN][50,75,100]) 27 different lists\n",
    "cnn_test_data = {'WGAN':[],'DCGAN':[],'CGAN':[]}\n",
    "cnn_pos_data = list() # cnn tested on only positive (\"1) data from dataset\n",
    "cnn_gan_data = list() # cnn tested on gan data (\"1\") labeled real\n",
    "cnn_pos_and_gan_data = list()# cnn tested on positive and real data labeled 1\n",
    "cnn_pos_vs_gan_data = list() # cnn tested on positive data (\"1\") vs generated data labeled (\"0\")\n",
    "cnn_pos_vs_neg_data = list() # \n",
    "cnn_gan_vs_neg_data = list() # \n",
    "cnn_pos_and_gan_vs_neg_data = list() #\n",
    "\n",
    "def testCNN():\n",
    "    for gan in ['WGAN','DCGAN']:\n",
    "        \n",
    "        \n",
    "        for keystroke_num in ['50']:\n",
    "            print(\"finish\")\n",
    "            cnn_pos_data = list() # cnn tested on only positive (\"1) data from dataset\n",
    "            cnn_gan_data = list() # cnn tested on gan data (\"1\") labeled real\n",
    "            cnn_pos_and_gan_data = list()# cnn tested on positive and real data labeled 1\n",
    "            cnn_pos_vs_gan_data = list() # cnn tested on positive data (\"1\") vs generated data labeled (\"0\")\n",
    "            cnn_pos_vs_neg_data = list() # \n",
    "            cnn_gan_vs_neg_data = list() # \n",
    "            cnn_pos_and_gan_vs_neg_data = list() #\n",
    "            path = r\"/home/jupyter/src/Thesis_Project/Data/\"+keystroke_num\n",
    "            all_data_files = glob.glob(path + \"/*\")\n",
    "            all_data_files.sort()\n",
    "            path = r\"/home/jupyter/src/Thesis_Project/\"+gan+\"_data/\"+keystroke_num\n",
    "            all_generated_data_files = glob.glob(path + \"/*\")\n",
    "            all_generated_data_files.sort()\n",
    "            path = r\"/home/jupyter/src/Thesis_Project/CNN_\" +keystroke_num+\"_models\"\n",
    "            all_cnns = glob.glob(path + \"/*\")\n",
    "            all_cnns.sort()\n",
    "           \n",
    "            for k in range(75):\n",
    "                # POS + GAN (i.e. both are labeled ones)\n",
    "                positive_data, gan_data = extractRealAndGeneratedData(k,all_data_files,all_generated_data_files)\n",
    "                total_dataset = np.concatenate((positive_data, gan_data), axis = 0)\n",
    "                total_labels = np.concatenate((\n",
    "                  np.ones((len(positive_data), 1)), \n",
    "                  np.ones((len(gan_data), 1))\n",
    "                ), axis=0)\n",
    "                test_transform = getTransform(total_dataset)\n",
    "                test_dataset = KeystrokesDataset(total_dataset, total_labels, test_transform)\n",
    "                test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "                #Loading and testing model \n",
    "                model.load_state_dict(torch.load(all_cnns[k]))\n",
    "                model.to(device)\n",
    "                model.eval()\n",
    "\n",
    "                # Test each CNN against Generated Data\n",
    "                test_acc = evaluate(model,test_dataloader)\n",
    "\n",
    "\n",
    "                # Summarize results \n",
    "\n",
    "                # fpr, tpr, thresholds = metrics.roc_curve(y, scores, pos_label=1)\n",
    "                # fnr = 1 - tpr\n",
    "                # eer_threshold = thresholds[np.nanargmin(np.absolute((fnr - fpr)))]\n",
    "                # eer = fpr[np.nanargmin(np.absolute((fnr - fpr)))]\n",
    "                # print(fpr,tpr,fnr)\n",
    "                # print('Accuracy of the network on the test inputs: %f %%' % (test_acc))\n",
    "                cnn_pos_and_gan_data.append(test_acc)\n",
    "\n",
    "\n",
    "                # POS ONLY\n",
    "                total_dataset = positive_data\n",
    "                total_labels = np.ones((len(positive_data), 1))\n",
    "                test_transform = getTransform(total_dataset)\n",
    "                test_dataset = KeystrokesDataset(total_dataset, total_labels, test_transform)\n",
    "                test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=True)\n",
    "                test_acc = evaluate(model,test_dataloader)\n",
    "                # print('Accuracy of the network on the test inputs: %f %%' % (test_acc))\n",
    "                cnn_pos_data.append(test_acc)\n",
    "\n",
    "\n",
    "                # GAN ONLY\n",
    "                total_dataset = gan_data\n",
    "                total_labels = np.ones((len(gan_data), 1))\n",
    "                test_transform = getTransform(total_dataset)\n",
    "                test_dataset = KeystrokesDataset(total_dataset, total_labels, test_transform)\n",
    "                test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=True)\n",
    "                test_acc = evaluate(model,test_dataloader)\n",
    "                # print('Accuracy of the network on the test inputs: %f %%' % (test_acc))\n",
    "                cnn_gan_data.append(test_acc)\n",
    "\n",
    "\n",
    "                # POS vs GAN ONLY\n",
    "                total_dataset = np.concatenate((positive_data, gan_data), axis = 0)\n",
    "                total_labels = np.concatenate((\n",
    "                  np.ones((len(positive_data), 1)), \n",
    "                  np.zeros((len(gan_data), 1))\n",
    "                ), axis=0)\n",
    "                test_transform = getTransform(total_dataset)\n",
    "                test_dataset = KeystrokesDataset(total_dataset, total_labels, test_transform)\n",
    "                test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=True)\n",
    "                test_acc = evaluate(model,test_dataloader)\n",
    "                # print('Accuracy of the network on the test inputs: %f %%' % (test_acc))\n",
    "                cnn_pos_vs_gan_data.append(test_acc)\n",
    "                \n",
    "                # GRAB  NEGATIVE DATA\n",
    "                negative_data = getNegative(k,all_data_files)\n",
    "    \n",
    "                # POS vs NEG \n",
    "                total_dataset = np.concatenate((positive_data, negative_data), axis = 0)\n",
    "                total_labels = np.concatenate((\n",
    "                  np.ones((len(positive_data), 1)), \n",
    "                  np.zeros((len(negative_data), 1))\n",
    "                ), axis=0)\n",
    "                test_transform = getTransform(total_dataset)\n",
    "                test_dataset = KeystrokesDataset(total_dataset, total_labels, test_transform)\n",
    "                test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=True)\n",
    "                test_acc = evaluate(model,test_dataloader)\n",
    "                # print('Accuracy of the network on the test inputs: %f %%' % (test_acc))\n",
    "                cnn_pos_vs_neg_data.append(test_acc)\n",
    "\n",
    "                \n",
    "                # GAN vs NEG\n",
    "                total_dataset = np.concatenate((gan_data, negative_data), axis = 0)\n",
    "                total_labels = np.concatenate((\n",
    "                  np.ones((len(gan_data), 1)), \n",
    "                  np.zeros((len(negative_data), 1))\n",
    "                ), axis=0)\n",
    "                test_transform = getTransform(total_dataset)\n",
    "                test_dataset = KeystrokesDataset(total_dataset, total_labels, test_transform)\n",
    "                test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=True)\n",
    "                test_acc = evaluate(model,test_dataloader)\n",
    "                # print('Accuracy of the network on the test inputs: %f %%' % (test_acc))\n",
    "                cnn_gan_vs_neg_data.append(test_acc)\n",
    "\n",
    "                \n",
    "                # POS + GAN vs NEG \n",
    "                total_dataset = np.concatenate((positive_data,np.concatenate((gan_data, negative_data), axis = 0)),axis=0)\n",
    "                total_labels = np.concatenate((\n",
    "                  np.ones((len(positive_data)+len(gan_data), 1)), \n",
    "                  np.zeros((len(negative_data), 1))\n",
    "                ), axis=0)\n",
    "                test_transform = getTransform(total_dataset)\n",
    "                test_dataset = KeystrokesDataset(total_dataset, total_labels, test_transform)\n",
    "                test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=True)\n",
    "                test_acc = evaluate(model,test_dataloader)\n",
    "                # print('Accuracy of the network on the test inputs: %f %%' % (test_acc))\n",
    "                cnn_pos_and_gan_vs_neg_data.append(test_acc)\n",
    "                \n",
    "                \n",
    "    \n",
    "            cnn_test_data[gan].append([cnn_pos_data,cnn_gan_data,cnn_pos_and_gan_data,cnn_pos_vs_gan_data,cnn_pos_vs_neg_data,cnn_gan_vs_neg_data, cnn_pos_and_gan_vs_neg_data])\n",
    "        \n",
    "\n",
    "testCNN()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4e619327-a74d-42e8-ae5f-b2b2506a695f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here\n",
      "~~~~~~~~~RESULTS~~~~~~~~~\n",
      "WGAN\t 50 keystrokes\t cnn_pos_data\t\t\tAVERAGE: 96.888\n",
      "WGAN\t 50 keystrokes\t cnn_gan_data\t\t\tAVERAGE: 88.688\n",
      "WGAN\t 50 keystrokes\t cnn_pos_and_gan_data\t\tAVERAGE: 97.376\n",
      "WGAN\t 50 keystrokes\t cnn_pos_vs_gan_data\t\tAVERAGE: 60.176\n",
      "WGAN\t 50 keystrokes\t cnn_pos_vs_neg_data\t\tAVERAGE: 93.532\n",
      "WGAN\t 50 keystrokes\t cnn_gan_vs_neg_data\t\tAVERAGE: 92.125\n",
      "WGAN\t 50 keystrokes\t cnn_pos_and_gan_vs_neg_data\tAVERAGE: 95.353\n",
      "\n",
      "\n",
      "\n",
      "DCGAN\t 50 keystrokes\t cnn_pos_data\t\t\tAVERAGE: 96.888\n",
      "DCGAN\t 50 keystrokes\t cnn_gan_data\t\t\tAVERAGE: 82.688\n",
      "DCGAN\t 50 keystrokes\t cnn_pos_and_gan_data\t\tAVERAGE: 97.383\n",
      "DCGAN\t 50 keystrokes\t cnn_pos_vs_gan_data\t\tAVERAGE: 60.183\n",
      "DCGAN\t 50 keystrokes\t cnn_pos_vs_neg_data\t\tAVERAGE: 93.646\n",
      "DCGAN\t 50 keystrokes\t cnn_gan_vs_neg_data\t\tAVERAGE: 92.135\n",
      "DCGAN\t 50 keystrokes\t cnn_pos_and_gan_vs_neg_data\tAVERAGE: 95.22\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Here\")\n",
    "test_name = [\"cnn_pos_data\\t\\t\\t\",\"cnn_gan_data\\t\\t\\t\",\"cnn_pos_and_gan_data\\t\\t\",\"cnn_pos_vs_gan_data\\t\\t\",\"cnn_pos_vs_neg_data\\t\\t\",\"cnn_gan_vs_neg_data\\t\\t\", \"cnn_pos_and_gan_vs_neg_data\\t\"]\n",
    "knum = ['50','50','100']\n",
    "print(\"~~~~~~~~~RESULTS~~~~~~~~~\")\n",
    "for g in ['WGAN','DCGAN']:\n",
    "    k = 0\n",
    "    for i in cnn_test_data[g]:\n",
    "        t = 0\n",
    "        for tests in i:\n",
    "            print(g +'\\t',knum[k]+\" keystrokes\\t\",test_name[t] + \"AVERAGE:\",round(sum(tests)/len(tests),3))\n",
    "            t += 1\n",
    "        k += 1\n",
    "        print(\"\\n\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20150c78-927c-4a1a-a70e-b2cc1d2b1973",
   "metadata": {},
   "source": [
    "#### positive_indices = list(range(3))\n",
    "positive = [[1,2,3],[1,1,1],[2,2,2]]\n",
    "np.random.shuffle(positive_indices)\n",
    "positive_data = positive[positive_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac164a28-f0f6-42ed-9e36-4caee78f48b8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/tmp/ipykernel_2330/1509070237.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m#Loading and testing model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_cnns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "import os \n",
    "cnn_pos_data = list() # cnn tested on only positive (\"1) data from dataset\n",
    "cnn_gan_data = list() # cnn tested on gan data (\"1\") labeled real\n",
    "cnn_pos_and_gan_data = list()# cnn tested on positive and real data labeled 1\n",
    "cnn_pos_vs_gan_data = list() # cnn tested on positive data (\"1\") vs generated data labeled (\"0\")\n",
    "cnn_pos_vs_neg_data = list() # \n",
    "cnn_gan_vs_neg_data = list() # \n",
    "cnn_pos_and_gan_vs_neg_data = list() #\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "path = r\"/home/jupyter/src/Thesis_Project/Data/100\"\n",
    "all_data_files = glob.glob(path + \"/*\")\n",
    "all_data_files.sort()\n",
    "path = r\"/home/jupyter/src/Thesis_Project/WGAN_data/100\"\n",
    "all_generated_data_files = glob.glob(path + \"/*\")\n",
    "all_generated_data_files.sort()\n",
    "path = r\"/home/jupyter/src/Thesis_Project/CNN_100_models\"\n",
    "all_cnns = glob.glob(path + \"/*\")\n",
    "all_cnns.sort()\n",
    "# print(all_generated_data_files)\n",
    "    \n",
    "\n",
    "#Loading and testing model \n",
    "model.load_state_dict(torch.load(all_cnns[0]))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "l=0\n",
    "for k in range(75):\n",
    "    # POS + GAN (i.e. both are labeled ones)\n",
    "    positive_data, g = extractRealAndGeneratedData(k,all_data_files,all_generated_data_files)\n",
    "    p, gan_data = extractRealAndGeneratedData(k,all_data_files,all_generated_data_files)\n",
    "    # are_same = np.array_equal(positive_data,p)\n",
    "    # are_same1 = np.array_equal(gan_data,g)\n",
    "    # if are_same:\n",
    "    #     print(\"The arrays are the same.\")\n",
    "    # else:\n",
    "    #     print(\"The arrays are different.\")\n",
    "    # if are_same1:\n",
    "    #     print(\"The arrays are the same.\")\n",
    "    # else:\n",
    "    #     print(\"The arrays are different.\")  \n",
    "\n",
    "    total_dataset = gan_data\n",
    "    total_labels = np.ones((len(gan_data), 1))\n",
    "    # total_dataset = np.concatenate((positive_data, gan_data), axis = 0)\n",
    "    # total_labels = np.concatenate((\n",
    "    #   np.ones((len(positive_data), 1)), \n",
    "    #   np.ones((len(gan_data), 1))\n",
    "    # ), axis=0)\n",
    "    test_transform = getTransform(total_dataset)\n",
    "    test_dataset = KeystrokesDataset(total_dataset, total_labels, test_transform)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "    #Loading and testing model \n",
    "    model.load_state_dict(torch.load(all_cnns[k]))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    test_acc = evaluate(model,test_dataloader)\n",
    "    l += test_acc\n",
    "\n",
    "\n",
    "    print('Accuracy of the network on the test inputs: %f %%' % (test_acc))\n",
    "print('Accuracy of the network on the test inputs: %f %%' % (l/48))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b263caac-70bd-41c8-bafc-3fd0f365636a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gen_data_0.npy', 'gen_data_000.npy', 'gen_data_1.npy', 'gen_data_001.npy', 'gen_data_2.npy', 'gen_data_002.npy', 'gen_data_3.npy', 'gen_data_003.npy', 'gen_data_4.npy', 'gen_data_004.npy', 'gen_data_5.npy', 'gen_data_005.npy', 'gen_data_6.npy', 'gen_data_006.npy', 'gen_data_007.npy', 'gen_data_7.npy', 'gen_data_8.npy', 'gen_data_008.npy', 'gen_data_009.npy', 'gen_data_9.npy', 'gen_data_10.npy', 'gen_data_010.npy', 'gen_data_11.npy', 'gen_data_011.npy', 'gen_data_012.npy', 'gen_data_12.npy', 'gen_data_13.npy', 'gen_data_013.npy', 'gen_data_14.npy', 'gen_data_014.npy', 'gen_data_15.npy', 'gen_data_015.npy', 'gen_data_016.npy', 'gen_data_16.npy', 'gen_data_17.npy', 'gen_data_017.npy', 'gen_data_18.npy', 'gen_data_018.npy', 'gen_data_019.npy', 'gen_data_19.npy', 'gen_data_20.npy', 'gen_data_020.npy', 'gen_data_21.npy', 'gen_data_021.npy', 'gen_data_022.npy', 'gen_data_22.npy', 'gen_data_23.npy', 'gen_data_023.npy', 'gen_data_024.npy', 'gen_data_24.npy', 'gen_data_025.npy', 'gen_data_25.npy', 'gen_data_26.npy', 'gen_data_026.npy', 'gen_data_027.npy', 'gen_data_27.npy', 'gen_data_28.npy', 'gen_data_028.npy', 'gen_data_29.npy', 'gen_data_029.npy', 'gen_data_030.npy', 'gen_data_30.npy', 'gen_data_31.npy', 'gen_data_031.npy', 'gen_data_032.npy', 'gen_data_32.npy', 'gen_data_33.npy', 'gen_data_033.npy', 'gen_data_034.npy', 'gen_data_34.npy', 'gen_data_035.npy', 'gen_data_35.npy', 'gen_data_036.npy', 'gen_data_36.npy', 'gen_data_37.npy', 'gen_data_037.npy', 'gen_data_38.npy', 'gen_data_038.npy', 'gen_data_039.npy', 'gen_data_39.npy', 'gen_data_40.npy', 'gen_data_040.npy', 'gen_data_41.npy', 'gen_data_041.npy', 'gen_data_42.npy', 'gen_data_042.npy', 'gen_data_043.npy', 'gen_data_43.npy', 'gen_data_44.npy', 'gen_data_044.npy', 'gen_data_45.npy', 'gen_data_045.npy', 'gen_data_46.npy', 'gen_data_046.npy', 'gen_data_47.npy', 'gen_data_047.npy', 'gen_data_048.npy', 'gen_data_48.npy', 'gen_data_049.npy', 'gen_data_49.npy', 'gen_data_50.npy', 'gen_data_050.npy', 'gen_data_51.npy', 'gen_data_051.npy', 'gen_data_052.npy', 'gen_data_52.npy', 'gen_data_053.npy', 'gen_data_53.npy', 'gen_data_54.npy', 'gen_data_054.npy', 'gen_data_055.npy', 'gen_data_55.npy', 'gen_data_56.npy', 'gen_data_056.npy', 'gen_data_057.npy', 'gen_data_57.npy', 'gen_data_58.npy', 'gen_data_058.npy', 'gen_data_059.npy', 'gen_data_59.npy', 'gen_data_60.npy', 'gen_data_060.npy', 'gen_data_61.npy', 'gen_data_061.npy', 'gen_data_062.npy', 'gen_data_62.npy', 'gen_data_063.npy', 'gen_data_63.npy', 'gen_data_64.npy', 'gen_data_064.npy', 'gen_data_065.npy', 'gen_data_65.npy', 'gen_data_066.npy', 'gen_data_66.npy', 'gen_data_067.npy', 'gen_data_67.npy', 'gen_data_068.npy', 'gen_data_68.npy', 'gen_data_069.npy', 'gen_data_69.npy', 'gen_data_70.npy', 'gen_data_070.npy', 'gen_data_71.npy', 'gen_data_071.npy', 'gen_data_72.npy', 'gen_data_072.npy', 'gen_data_073.npy', 'gen_data_73.npy', 'gen_data_74.npy', 'gen_data_074.npy']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "directory = '/home/jupyter/src/Thesis_Project/WGAN_data/50/'\n",
    "prefix = 'gen_data_'\n",
    "extension = '.npy'\n",
    "\n",
    "#Get the list of files to be renamed\n",
    "files = os.listdir(directory)\n",
    "files = [f for f in files if f.startswith(prefix) and f.endswith(extension)]\n",
    "files = sorted(files, key=lambda x: int(x.split('.')[0].split('_')[-1]))\n",
    "print(files)\n",
    "\n",
    "#Rename the files\n",
    "for i, f in enumerate(files):\n",
    "    old_name = os.path.join(directory, f)\n",
    "    new_name = os.path.join(directory, prefix + str(i).zfill(3) + extension)\n",
    "    os.rename(old_name, new_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "81076fe9-a770-4ced-8664-ade0ba35d269",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model,test_dataloader):\n",
    "    correct = 0.0\n",
    "    total = 0.0\n",
    "\n",
    "    scores = []\n",
    "    y = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(test_dataloader):\n",
    "            inputs, labels = data\n",
    "            #images = images.to(device).half() # uncomment for half precision model\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            #Passes input through CNN and generates output\n",
    "            outputs = model.forward(inputs, training=False)\n",
    "            total += labels.size(0)\n",
    "\n",
    "            scores.extend(outputs.cpu().numpy().reshape(len(outputs)))\n",
    "            y.extend(labels.cpu().numpy().reshape(len(labels)))\n",
    "            \n",
    "            #scores are then aggregated and averaged\n",
    "            predicted = (outputs > 0.3).float()\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    test_acc = (100.0 * correct) / total\n",
    "    return test_acc\n",
    "\n",
    "def getTransform(total_dataset):\n",
    "    mean = 0.\n",
    "    std = 0.\n",
    "\n",
    "    # batch size (the last batch can have smaller size!)\n",
    "    batch_samples = len(total_dataset) \n",
    "    images = total_dataset.reshape(batch_samples, 5, -1)\n",
    "    mean += images.mean(2).sum(0)\n",
    "    std += images.std(2).sum(0)\n",
    "\n",
    "    mean /= len(total_dataset)\n",
    "    std /= len(total_dataset)\n",
    "\n",
    "\n",
    "    test_transform = A.Compose([\n",
    "    A.Normalize(mean=mean.tolist(), std=std.tolist(), max_pixel_value=1.0, p=1.0),\n",
    "    ToTensorV2(p=1.0),\n",
    "    ], p=1)\n",
    "    return test_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac72f0f-a2ec-4fbc-ac6d-1fd5bbef6dc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2977cb-a703-43cc-bbff-16f6c0ba3e36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.1-13.m103",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-13:m103"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
