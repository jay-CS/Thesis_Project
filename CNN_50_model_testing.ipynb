{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8796c08-7b0c-4b51-8171-78470f103f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from sklearn import metrics\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import math\n",
    "import random\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e28cfee0-1873-4557-a944-890f2d02c481",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# print(\"Total number of subjects:\", len(all_data_files))\n",
    "# print(\"Total number of subjects:\", len(all_generated_data_files))\n",
    "# print(all_generated_data_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0a7e339-b04e-4c81-98b6-1796aa2b4af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KeystrokesDataset(Dataset):\n",
    "    def __init__(self, samples, labels, transform):\n",
    "        self.samples = samples\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        x = self.samples[idx]\n",
    "        x = x.transpose((1, 2 ,0))\n",
    "        x = self.transform(image=x)['image']\n",
    "        label = torch.from_numpy(self.labels[idx]).float()\n",
    "        return x, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27c2cb46-dbda-411d-8480-2a252ca1df4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KeystrokeImageNetwork(nn.Module):\n",
    "  \n",
    "    def __init__(self):\n",
    "        super(KeystrokeImageNetwork, self).__init__()\n",
    "\n",
    "        # 10 fingers\n",
    "        # => output (x, 10)\n",
    "        self.conv1_1 = nn.Conv2d(5, 64, kernel_size=3, padding=1)\n",
    "        self.conv1_2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "\n",
    "        self.conv2_1 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.conv2_2 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "\n",
    "        # self.conv3_1 = nn.Conv2d(5, 64, kernel_size=(1,42), padding=0)\n",
    "        # self.conv4_1 = nn.Conv2d(5, 64, kernel_size=(43,1), padding=0)\n",
    "\n",
    "        # self.conv3_1 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        # self.conv3_2 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "        # self.conv3_3 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "\n",
    "        # max pooling (kernel_size, stride)\n",
    "        # self.pool = nn.AvgPool2d(2, 2)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        # fully conected layers:\n",
    "        # self.fc6 = nn.Linear(42*42*5, 512)\n",
    "        self.fc6 = nn.Linear(10*10*128, 512)\n",
    "        # self.fc6 = nn.Linear(10*10*128 + 64*43 + 64*42, 512)\n",
    "        self.fc7 = nn.Linear(512, 64)\n",
    "        self.fc8 = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x, training=True):\n",
    "        # beforeX = F.relu(self.conv3_1(x))\n",
    "        # afterX = F.relu(self.conv4_1(x))\n",
    "\n",
    "        # print(\"beofore x shape is %s, after x shape is %s\"%(beforeX.shape, afterX.shape)) #64 x 64 x 43 x 1 #64 x 64 x 1 x 42\n",
    "\n",
    "        # probelm of the sparse feature\n",
    "        # need to be fixed in the feature engineering\n",
    "        # Add two new channel \n",
    "        # beforeX = beforeX.view(-1, 64 * 43 * 1)  # represent bigraph (*) - (correspending key)\n",
    "        # afterX = afterX.view(-1, 64 * 1 * 42) # represent bigraph (corresponding key) - (*)\n",
    "\n",
    "        x = F.relu(self.conv1_1(x))\n",
    "        x = F.relu(self.conv1_2(x))\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = F.relu(self.conv2_1(x))\n",
    "        x = F.relu(self.conv2_2(x))\n",
    "        x = self.pool(x)\n",
    "\n",
    "        # x = F.relu(self.conv3_1(x))\n",
    "        # x = F.relu(self.conv3_2(x))\n",
    "        # x = F.relu(self.conv3_3(x))\n",
    "        # x = self.pool(x)\n",
    "\n",
    "        # x = x.view(-1, 42*42*5)\n",
    "        x = x.view(-1, 10*10*128)\n",
    "\n",
    "        # x = torch.cat((x, beforeX, afterX), 1)\n",
    "        # x = F.dropout(x, 0.5, training=training)\n",
    "        x = F.relu(self.fc6(x))\n",
    "        x = F.dropout(x, 0.5, training=training)\n",
    "        x = F.relu(self.fc7(x))\n",
    "        # x = F.dropout(x, 0.5, training=training)\n",
    "        x = self.fc8(x)\n",
    "\n",
    "        # x = F.log_softmax(x, dim=1)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b925c90-336e-49f2-b353-e40d8e0ddb7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#EXTRACTS REAL AND GENERATED SAMPLES\n",
    "def extractRealAndGeneratedData(user_num, all_data_files,all_generated_data_files, positive_only = False, gan_only = False):\n",
    "    positive_data = None\n",
    "    negative_data = None\n",
    "\n",
    "    positive_init = False\n",
    "    negative_init = False\n",
    "\n",
    "    positive_indices = None\n",
    "    negative_indices = None\n",
    "\n",
    "    positive_data = list()\n",
    "    negative_data = list()\n",
    "    target = user_num\n",
    "    total = 40000\n",
    "    \n",
    "    #print('**********************', target)\n",
    "    \n",
    "    if(positive_only):\n",
    "        for i, filename in enumerate(all_data_files):\n",
    "            if i == target:\n",
    "                positive_dataset = np.load(filename)\n",
    "                positive_indices = list(range(len(positive_dataset)))\n",
    "                np.random.shuffle(positive_indices)\n",
    "                positive_init = True\n",
    "                positive_data = positive_dataset[positive_indices]\n",
    "                #print(\"Current positive keystroke images Data shape is\",positive_data.shape)\n",
    "        return positive_data\n",
    "    \n",
    "    if(gan_only):\n",
    "        for i, filename in enumerate(all_generated_data_files):\n",
    "            if i == target:\n",
    "                negative_dataset = np.load(filename)\n",
    "                negative_indices = list(range(len(negative_dataset)))\n",
    "                np.random.shuffle(negative_indices)\n",
    "                negative_init = True\n",
    "                negative_data = negative_dataset[negative_indices]\n",
    "                #print(\"Current negative keystroke images Data shape is\",negative_data.shape)\n",
    "        return negative_data\n",
    "        \n",
    "    else:\n",
    "        for i, filename in enumerate(all_data_files):\n",
    "            if i == target:\n",
    "                positive_dataset = np.load(filename)\n",
    "                positive_indices = list(range(len(positive_dataset)))\n",
    "                np.random.shuffle(positive_indices)\n",
    "                positive_init = True\n",
    "                positive_data = positive_dataset[positive_indices]\n",
    "                #print(\"Current positive keystroke images Data shape is\",positive_data.shape)\n",
    "    \n",
    "        for i, filename in enumerate(all_generated_data_files):\n",
    "            if i == target:\n",
    "                negative_dataset = np.load(filename)\n",
    "                negative_indices = list(range(len(negative_dataset)))\n",
    "                np.random.shuffle(negative_indices)\n",
    "                negative_init = True\n",
    "                negative_data = negative_dataset[negative_indices]\n",
    "                #print(\"Current negative keystroke images Data shape is\",negative_data.shape)\n",
    "        return positive_data, negative_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ddea05fb-d709-454e-9099-c6702e8c11fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNegative(target,all_data_files):\n",
    "    positive_length = 100\n",
    "    negative_length = 35000\n",
    "    negative_data = None\n",
    "    negative_init = False\n",
    "        \n",
    "    for i, filename in enumerate(all_data_files):\n",
    "            if i != target:\n",
    "                negative_dataset = np.load(filename)\n",
    "                nega_len = len(negative_dataset)\n",
    "                if random.randint(0, 1) == 0:\n",
    "                    smaple_len = math.floor(nega_len/negative_length * positive_length)\n",
    "                else:\n",
    "                    smaple_len = math.ceil(nega_len/negative_length * positive_length)\n",
    "                negative_indices = list(range(nega_len))\n",
    "\n",
    "                if not negative_init:\n",
    "                    negative_data = negative_dataset[negative_indices[:smaple_len]]\n",
    "                    negative_init = True\n",
    "                else:\n",
    "                    extend_sameple = negative_dataset[negative_indices[:smaple_len]]\n",
    "                    negative_data = np.concatenate((negative_data, extend_sameple), axis=0)\n",
    "    #print(\"Negative Data Shape:\",negative_data.shape)\n",
    "    return negative_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b94a7e0d-f8b8-4fdd-8070-786b0c53ba4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model,test_dataloader):\n",
    "    correct = 0.0\n",
    "    total = 0.0\n",
    "\n",
    "    scores = []\n",
    "    y = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(test_dataloader):\n",
    "            inputs, labels = data\n",
    "            #images = images.to(device).half() # uncomment for half precision model\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            #Passes input through CNN and generates output\n",
    "            outputs = model.forward(inputs, training=False)\n",
    "            total += labels.size(0)\n",
    "\n",
    "            scores.extend(outputs.cpu().numpy().reshape(len(outputs)))\n",
    "            y.extend(labels.cpu().numpy().reshape(len(labels)))\n",
    "            \n",
    "            #scores are then aggregated and averaged\n",
    "            predicted = (outputs > 0.3).float()\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    test_acc = (100.0 * correct) / total\n",
    "    return test_acc\n",
    "\n",
    "def getTransform(total_dataset):\n",
    "    mean = 0.\n",
    "    std = 0.\n",
    "\n",
    "    # batch size (the last batch can have smaller size!)\n",
    "    batch_samples = len(total_dataset) \n",
    "    images = total_dataset.reshape(batch_samples, 5, -1)\n",
    "    mean += images.mean(2).sum(0)\n",
    "    std += images.std(2).sum(0)\n",
    "\n",
    "    mean /= len(total_dataset)\n",
    "    std /= len(total_dataset)\n",
    "\n",
    "\n",
    "    test_transform = A.Compose([\n",
    "    A.Normalize(mean=mean.tolist(), std=std.tolist(), max_pixel_value=1.0, p=1.0),\n",
    "    ToTensorV2(p=1.0),\n",
    "    ], p=1)\n",
    "    return test_transform\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3154400-42c2-42ec-93c5-25db34dc1583",
   "metadata": {},
   "outputs": [],
   "source": [
    "## EVALUATING GENERATED DATA  \n",
    "model = KeystrokeImageNetwork()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "knum = 0\n",
    "# list of test results to save and average at the end (CNN [50,75,100] vs [WGAN,DCGAN,CGAN][50,75,100]) 27 different lists\n",
    "cnn_test_data = {'WGAN':[],'DCGAN':[],'CGAN':[]}\n",
    "cnn_pos_data = list() # cnn tested on only positive (\"1) data from dataset\n",
    "cnn_gan_data = list() # cnn tested on gan data (\"1\") labeled real\n",
    "cnn_pos_and_gan_data = list()# cnn tested on positive and real data labeled 1\n",
    "cnn_pos_vs_gan_data = list() # cnn tested on positive data (\"1\") vs generated data labeled (\"0\")\n",
    "cnn_pos_vs_neg_data = list() # \n",
    "cnn_gan_vs_neg_data = list() # \n",
    "cnn_pos_and_gan_vs_neg_data = list() #\n",
    "\n",
    "def testCNN():\n",
    "    for gan in ['WGAN','DCGAN']:\n",
    "        \n",
    "        \n",
    "        for keystroke_num in ['50','75','100']:\n",
    "            cnn_pos_data = list() # cnn tested on only positive (\"1) data from dataset\n",
    "            cnn_gan_data = list() # cnn tested on gan data (\"1\") labeled real\n",
    "            cnn_pos_and_gan_data = list()# cnn tested on positive and real data labeled 1\n",
    "            cnn_pos_vs_gan_data = list() # cnn tested on positive data (\"1\") vs generated data labeled (\"0\")\n",
    "            cnn_pos_vs_neg_data = list() # \n",
    "            cnn_gan_vs_neg_data = list() # \n",
    "            cnn_pos_and_gan_vs_neg_data = list() #\n",
    "            path = r\"/home/jupyter/src/Thesis_Project/Data/\"+keystroke_num\n",
    "            all_data_files = glob.glob(path + \"/*\")\n",
    "            all_data_files.sort()\n",
    "            path = r\"/home/jupyter/src/Thesis_Project/\"+gan+\"_data/\"+keystroke_num\n",
    "            all_generated_data_files = glob.glob(path + \"/*\")\n",
    "            all_generated_data_files.sort()\n",
    "            \n",
    "            for k in range(75):\n",
    "                # POS + GAN (i.e. both are labeled ones)\n",
    "                positive_data, gan_data = extractRealAndGeneratedData(k,all_data_files,all_generated_data_files)\n",
    "                total_dataset = np.concatenate((positive_data, gan_data), axis = 0)\n",
    "                total_labels = np.concatenate((\n",
    "                  np.ones((len(positive_data), 1)), \n",
    "                  np.ones((len(gan_data), 1))\n",
    "                ), axis=0)\n",
    "                test_transform = getTransform(total_dataset)\n",
    "                test_dataset = KeystrokesDataset(total_dataset, total_labels, test_transform)\n",
    "                test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "                #Loading and testing model \n",
    "                model.load_state_dict(torch.load(\"/home/jupyter/src/Thesis_Project/CNN_\"+keystroke_num+\"_models/model_\"+str(k)+\".pth\"))\n",
    "                model.to(device)\n",
    "                model.eval()\n",
    "\n",
    "                # Test each CNN against Generated Data\n",
    "                test_acc = evaluate(model,test_dataloader)\n",
    "\n",
    "\n",
    "                # Summarize results \n",
    "\n",
    "                # fpr, tpr, thresholds = metrics.roc_curve(y, scores, pos_label=1)\n",
    "                # fnr = 1 - tpr\n",
    "                # eer_threshold = thresholds[np.nanargmin(np.absolute((fnr - fpr)))]\n",
    "                # eer = fpr[np.nanargmin(np.absolute((fnr - fpr)))]\n",
    "                # print(fpr,tpr,fnr)\n",
    "                # print('Accuracy of the network on the test inputs: %f %%' % (test_acc))\n",
    "                cnn_pos_and_gan_data.append(test_acc)\n",
    "\n",
    "\n",
    "                # POS ONLY\n",
    "                total_dataset = positive_data\n",
    "                total_labels = np.ones((len(positive_data), 1))\n",
    "                test_transform = getTransform(total_dataset)\n",
    "                test_dataset = KeystrokesDataset(total_dataset, total_labels, test_transform)\n",
    "                test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=True)\n",
    "                test_acc = evaluate(model,test_dataloader)\n",
    "                # print('Accuracy of the network on the test inputs: %f %%' % (test_acc))\n",
    "                cnn_pos_data.append(test_acc)\n",
    "\n",
    "\n",
    "                # GAN ONLY\n",
    "                total_dataset = gan_data\n",
    "                total_labels = np.ones((len(gan_data), 1))\n",
    "                test_transform = getTransform(total_dataset)\n",
    "                test_dataset = KeystrokesDataset(total_dataset, total_labels, test_transform)\n",
    "                test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=True)\n",
    "                test_acc = evaluate(model,test_dataloader)\n",
    "                # print('Accuracy of the network on the test inputs: %f %%' % (test_acc))\n",
    "                cnn_gan_data.append(test_acc)\n",
    "\n",
    "\n",
    "                # POS vs GAN ONLY\n",
    "                total_dataset = np.concatenate((positive_data, gan_data), axis = 0)\n",
    "                total_labels = np.concatenate((\n",
    "                  np.ones((len(positive_data), 1)), \n",
    "                  np.zeros((len(gan_data), 1))\n",
    "                ), axis=0)\n",
    "                test_transform = getTransform(total_dataset)\n",
    "                test_dataset = KeystrokesDataset(total_dataset, total_labels, test_transform)\n",
    "                test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=True)\n",
    "                test_acc = evaluate(model,test_dataloader)\n",
    "                # print('Accuracy of the network on the test inputs: %f %%' % (test_acc))\n",
    "                cnn_pos_vs_gan_data.append(test_acc)\n",
    "                \n",
    "                # grab negative data\n",
    "                negative_data = getNegative(k,all_data_files)\n",
    "    \n",
    "                # POS vs NEG \n",
    "                total_dataset = np.concatenate((positive_data, negative_data), axis = 0)\n",
    "                total_labels = np.concatenate((\n",
    "                  np.ones((len(positive_data), 1)), \n",
    "                  np.zeros((len(negative_data), 1))\n",
    "                ), axis=0)\n",
    "                test_transform = getTransform(total_dataset)\n",
    "                test_dataset = KeystrokesDataset(total_dataset, total_labels, test_transform)\n",
    "                test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=True)\n",
    "                test_acc = evaluate(model,test_dataloader)\n",
    "                # print('Accuracy of the network on the test inputs: %f %%' % (test_acc))\n",
    "                cnn_pos_vs_neg_data.append(test_acc)\n",
    "\n",
    "                \n",
    "                # GAN vs NEG\n",
    "                total_dataset = np.concatenate((gan_data, negative_data), axis = 0)\n",
    "                total_labels = np.concatenate((\n",
    "                  np.ones((len(gan_data), 1)), \n",
    "                  np.zeros((len(negative_data), 1))\n",
    "                ), axis=0)\n",
    "                test_transform = getTransform(total_dataset)\n",
    "                test_dataset = KeystrokesDataset(total_dataset, total_labels, test_transform)\n",
    "                test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=True)\n",
    "                test_acc = evaluate(model,test_dataloader)\n",
    "                # print('Accuracy of the network on the test inputs: %f %%' % (test_acc))\n",
    "                cnn_gan_vs_neg_data.append(test_acc)\n",
    "\n",
    "                \n",
    "                # POS + GAN vs NEG \n",
    "                total_dataset = np.concatenate((positive_data,np.concatenate((gan_data, negative_data), axis = 0)),axis=0)\n",
    "                total_labels = np.concatenate((\n",
    "                  np.ones((len(positive_data)+len(gan_data), 1)), \n",
    "                  np.zeros((len(negative_data), 1))\n",
    "                ), axis=0)\n",
    "                test_transform = getTransform(total_dataset)\n",
    "                test_dataset = KeystrokesDataset(total_dataset, total_labels, test_transform)\n",
    "                test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=True)\n",
    "                test_acc = evaluate(model,test_dataloader)\n",
    "                # print('Accuracy of the network on the test inputs: %f %%' % (test_acc))\n",
    "                cnn_pos_and_gan_vs_neg_data.append(test_acc)\n",
    "                \n",
    "                \n",
    "    \n",
    "            cnn_test_data[gan].append([cnn_pos_data,cnn_gan_data,cnn_pos_and_gan_data,cnn_pos_vs_gan_data,cnn_pos_vs_neg_data,cnn_gan_vs_neg_data, cnn_pos_and_gan_vs_neg_data])\n",
    "        \n",
    "\n",
    "testCNN()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e619327-a74d-42e8-ae5f-b2b2506a695f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_name = [\"cnn_pos_data\\t\\t\\t\",\"cnn_gan_data\\t\\t\\t\",\"cnn_pos_and_gan_data\\t\\t\",\"cnn_pos_vs_gan_data\\t\\t\",\"cnn_pos_vs_neg_data\\t\\t\",\"cnn_gan_vs_neg_data\\t\\t\", \"cnn_pos_and_gan_vs_neg_data\\t\"]\n",
    "knum = ['50','75','75']\n",
    "for g in ['WGAN','DCGAN']:\n",
    "    k = 0\n",
    "    for i in cnn_test_data[g]:\n",
    "        t = 0\n",
    "        for tests in i:\n",
    "            print(g +'\\t',knum[k]+\" keystrokes\\t\",test_name[t] + \"AVERAGE:\",round(sum(tests)/len(tests),3))\n",
    "            t += 1\n",
    "        k += 1\n",
    "        print(\"\\n\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20150c78-927c-4a1a-a70e-b2cc1d2b1973",
   "metadata": {},
   "source": [
    "#### positive_indices = list(range(3))\n",
    "positive = [[1,2,3],[1,1,1],[2,2,2]]\n",
    "np.random.shuffle(positive_indices)\n",
    "positive_data = positive[positive_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ac164a28-f0f6-42ed-9e36-4caee78f48b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_activation_statistics(images, model):\n",
    "    # Convert images to tensor and normalize\n",
    "    images = torch.tensor(images).float()\n",
    "    images = images.to(device)\n",
    "    images = (images - 0.5) * 2\n",
    "\n",
    "    # Run images through model\n",
    "    with torch.no_grad():\n",
    "        activations = model(images)\n",
    "        activations = activations.view(activations.size(0), -1)\n",
    "    \n",
    "    # Calculate mean and covariance of activations\n",
    "    mean = torch.mean(activations, dim=0)\n",
    "    cov = torch.matmul(activations.T, activations) / activations.shape[0] - torch.matmul(mean.unsqueeze(1), mean.unsqueeze(0))\n",
    "\n",
    "    return mean.cpu().numpy(), cov.cpu().numpy()\n",
    "\n",
    "def calculate_fid_score(images1, images2, model):\n",
    "    # Calculate activations and statistics for each dataset\n",
    "    mu1, sigma1 = calculate_activation_statistics(images1, model)\n",
    "    mu2, sigma2 = calculate_activation_statistics(images2, model)\n",
    "\n",
    "    # Calculate the squared difference between means\n",
    "    diff = mu1 - mu2\n",
    "    diff_squared = diff.dot(diff)\n",
    "\n",
    "    # Calculate the trace of the product of covariance matrices\n",
    "    covmean = torch.from_numpy(scipy.linalg.sqrtm(sigma1.dot(sigma2))).real\n",
    "    if not np.isfinite(covmean).all():\n",
    "        covmean = torch.eye(sigma1.shape[0])\n",
    "\n",
    "    # Calculate the FID score\n",
    "    fid = diff_squared + torch.trace(sigma1 + sigma2 - 2 * covmean)\n",
    "\n",
    "    return fid.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b263caac-70bd-41c8-bafc-3fd0f365636a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81076fe9-a770-4ced-8664-ade0ba35d269",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac72f0f-a2ec-4fbc-ac6d-1fd5bbef6dc5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.1-13.m103",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-13:m103"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
